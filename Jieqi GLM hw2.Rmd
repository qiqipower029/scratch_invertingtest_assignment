---
title: "GLM HW2 Jieqi"
author: "Jieqi Tu"
date: "2/27/2021"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

## 4.36

```{r Q4.36}
set.seed(3)
n <- 25
x <- runif(n, 0, 100)
sigma <- 10
error <- rnorm(n, 0, sd = sigma)
y <- 45 + 0.1*x + 5e-4*x^2 + 5e-7*x^3 + 5e-10*x^4 + 5e-13*x^5 + error
true_y <- y - error
## simple model
simple_model <- lm(y~x)
## correct model
correct_model <- lm(y~poly(x, 5, raw = T))
## visualization
xy <- data.frame(x,y,true_y=true_y)

xy %>% ggplot() + 
  aes(x = x, y = y) + 
  geom_point() + 
  geom_smooth(method = "lm", formula = y~x, se = F, aes(color="Simple model")) + 
  geom_smooth(method = "lm", formula = y~poly(x, 5, raw = T), se = F, aes(color="Correct model")) +
  geom_smooth(aes(x = x, y = true_y, se = F, color="True relationship"), size = 1) +
  scale_colour_manual(name="legend", values=c("blue", "red", "green")) + theme_bw()


## quality of fit
mean(abs(simple_model$fitted.values - y))
mean(abs(correct_model$fitted.values - y))
```

Correct model achieved a better fit but has a problem of overfitting, since there are only 25 samples but it used 6 parameters to describe the relationship, where the true coefficients of high-order terms are almost zero. So this resulted in high complexity of the model Complex model has a better fit but performs bad in prediction. Simple model may has higher bias, but does better in prediction. Therefore, model parsimony sometimes provide similar prediction performance but much more interpretability.

## 4.37
```{r Q4.37 1}
summary(simple_model)
summary(correct_model)
```

From the results, we could see that the estimated coefficients for $x^2$ and higher orders were not significant, and the standard error of these estimated coefficients were large. This inflated variation was caused by collinearity.
```{r Q4.37 2}
x2 = x^2
x3 = x^3
x4 = x^4
x5 = x^5
data = data.frame(x, x2, x3, x4, x5)
cor(data)
```

Then we looked into the correlation matrix of predictors in the "correct" model, we could see that the correlation between variables are very high (they are all greater than 0.8, and most of them are greater than 0.9). This can cause collinearity.

## 4.38
```{r Q4.38}
set.seed(1029)
x <- runif(100, 0, 100)
y <- runif(100, 0, 100)
# calculate true R^2
r <- matrix(0, 99, 3)
r[1, 1] <- summary(lm(y ~ x))$'r.squared'
r[1, 2] <- summary(lm(y ~ x))$'adj.r.squared'
r[1, 3] <- summary(lm(y ~ x))$coefficients[1, 4]
for (i in 2:99) {
r[i, 1] <- summary(lm(y ~ poly(x, i, raw = T)))$'r.squared'
r[i, 2] <- summary(lm(y ~ poly(x, i, raw = T)))$'adj.r.squared'
r[i, 3] <- summary(lm(y ~ poly(x, i, raw = T)))$coefficients[1, 4]
}
colnames(r) <- c('r.squared', 'adj.r.squared', 'intercept sig')
plot(r[,1], type = 'l', xlab = 'Degree', ylab = 'R.squared', main = 'Distribution of R.squared with increasing degrees of x')
plot(r[,2], type = 'l', xlab = 'Degree', ylab = 'Adj.R.squared', main = 'Distribution of Adjusted R.squared with increasing degrees of x')
plot(r[,3], type = 'l', xlab = 'Degree', ylab = 'Significance of Intercept', main = 'Distribution of Significance of Intercept with increasing in degrees of x')
abline(h = 0.05, col = 'red', lty = 2)
```

From the plot, we could see that, the $R^2$ increases as the degrees of x increases. However, although the theoretical value of $R^2$ should achieve 1 when the degree of x is 99, our result only have the value of $R^2$ less than 0.30 when the degree is 99. This is due to the multicollinearity when p reaches approximately 15.
The adjusted $R^2$ and the p-values for intercept term have more unstable performance. Only a few models have significant intercepts. 